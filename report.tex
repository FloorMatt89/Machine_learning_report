\documentclass[10pt]{article}

%\usepackage{hyperref}
\usepackage[left=0.8in,right=0.8in,top=0.15in,bottom=0.8in]{geometry}
\usepackage{xcolor}


%\hypersetup{colorlinks=true,linkcolor=blue,urlcolor=blue}
%\urlstyle{rm}
\usepackage{url}


\title{CAI 4104: Machine Learning Engineering\\
	\large Project Report:  {\textcolor{purple}{Image Classification Using a CNN}}} %% TODO: replace with the title of your project
	
	
	
%% TODO: your name and email go here (all members of the group)
%% Comment out as needed and designate a point of contact
%% Add / remove names as necessary
\author{
        Sebastian Villamizar \\
        villamizar.s@ufl.edu\\
        \and
        Jason Chen \\
        jchen8@ufl.edu\\
        \and
        Matthew Segura \\
        matthewsegura@ufl.edu\\
        \and
        Adrian Lehnhaeuser \\
        alehnhaeuser@ufl.edu\\
        \and
        Logan Dukes\\
        logan.dukes@ufl.edu\\
}


% set the date to today
\date{\today}


\begin{document} % start document tag

\maketitle



%%% Remember: writing counts! (try to be clear and concise.)
%%% Make sure to cite your sources and references (use refs.bib and \cite{} or \footnote{} for URLs).
%%%



%% TODO: write an introduction to make the report self-contained
%% Must address:
%% - What is the project about and what is notable about the approach and results?
%%
\section{Introduction}
The final project for CAI4104 is about implementing a complete ML pipeline for object classification from images of everyday objects. We classified 12 objects: pen, paper, book, clock, phone, laptop, chair, desk, water bottle, keychain, backpack, calculator. To accomplish this, we used a Convolutional Neural Network because it is well-suited to computer tasks on image data. CNNs have high memory usage during training so we leveraged computing power from HiPerGator and Google Colab. TODO: ADD SIMPLE RESULTS
% TODO:
%Write here. You can cite scholarly work like this~\cite{murphy2022probabilistic}. You can also include URLs in a footnote %like this.\footnote{UCI repository: \url{https://archive.ics.uci.edu/}.}




%% TODO: write about your approach / ML pipeline
%% Must contain:
%% - How you are trying to solve this problem
%% - How did you process the data?
%% - What is the task and approach (ML techniques)?
%%
\section{Approach}
To implement our pipeline we imported several python libraries: numpy, pandas, tensorflow, sklearn, and pytorch. These libraries helped us process the data and train the model. First, we applied label encoding to convert the object classes into integer class labels ranging from 0 to 11. These integer labels are compatible with the Sparse Categorical Cross Entropy loss function used in training our CNN model. To load and preprocess the image data we iterated over each image file and added them to a numpy array X, our feature matrix, and the labels to Y. Since the images have RGB values that range from 0 to 255, we normalized the data by dividing each pixel value by 255 to get them in the range [0, 1]. This is important because it accelerates convergence and improves numerical stability by stabilizing the gradients calculated by the optimizer, Adam. Then we used a standard seed, 42, and an 80, 10, 10 percent training, test, and validation split. We tested this data on a simple CNN architecture using tensorflow. 

The architecture consists of 3 convolutional layers with filter size 3x3, increasing channel depth (32, 64, 128), each with ReLU activation and max-pooling layers. This is followed by a flattening layer, a dense layer with 256 units, a drouput layer with a 0.5 rate, and finally a dense output layer with 12 units and softmax activation. After training this model over 30 epochs using the adam optimizer and SparseCategoricalCrossentropy loss function, we achieved a training accuracy of ~97\% and a loss of 0.0755. But when we evaluaed it on the unseen test data, the accuracy dropped significantly to 56\%. Therefore, our model was overfitting due to a lack of data augmentation. To augment the data we used a YouTube video by codebasics to guide us\footnote{Data augmentation to address overfitting: \url{https://www.youtube.com/watch?v=mTVf7BN7S8w&list=WL&index=2&t=1601s}.} For every input we generated multiple new samples using edge enhancement, smoothing, grayscale, gaussian blur, oversaturation, and color inversion. This increased amount of images from 4757 to 57084. 






%% TODO: write about your evaluation methodology
%% Must contain:
%% - What are the metrics?
%% - What are the baselines?
%% - How did you split the data?
%%
\section{Evaluation Methodology}

% TODO:
We used accuracy as our main metric (number of correct labels / total number of examples). A simple baseline to use would be to always guess the most common class. In the given dataset, 
the water bottle class had the highest number of examples (418) and the total number of images was 4,757, so a baseline accuracy would be 
\begin{equation}
        \frac{418}{4,757} \approx 0.0878705066218205.
\end{equation}
We split did an 80--10--10 train-validation-test set split.


%% TODO: write about your results/findings
%% Must contain:
%% - results comparing your approach to baseline according to metrics
%%
%% You can present this information in whatever way you want but consider tables and (or) figures.
%%
\section{Results}

% TODO:
\subsection{Training Process and Model Behavior}

The convolutional neural network (CNN) was trained on an image dataset over 15 epochs using Keras and TensorFlow. To address the relatively small size of the original dataset, extensive data augmentation was applied through the \texttt{ImageDataGenerator} class, including rotation, shifting, flipping, and color-based transformations. The use of these techniques expanded the dataset from approximately 4,757 images to over 57,000 images, providing the model with greater variety and reducing the risk of overfitting.

Throughout training, the model showed steady improvements in both training and validation metrics. Training accuracy increased rapidly in the initial epochs and continued to improve more gradually in later stages, ultimately reaching approximately 98\%. Validation accuracy closely mirrored the training trend, plateauing around 95\%. Simultaneously, training and validation loss values decreased consistently, indicating that the model successfully minimized the Sparse Categorical Cross-Entropy loss function used during optimization.

The architecture of the model—consisting of three convolutional layers with increasing depth, followed by a flattening and dense output layer—proved effective in extracting hierarchical feature representations from the images. The inclusion of dropout regularization at a rate of 0.5 further helped prevent overfitting by randomly deactivating neurons during training, encouraging the network to learn more robust feature combinations.

\subsection{Model Evaluation and Test Set Performance}

After completing training, the final model was evaluated on a held-out test set consisting of previously unseen examples. The model achieved a final test accuracy of approximately 94\%, which was closely aligned with the validation accuracy observed during training. This consistency between validation and test results highlights the model's ability to generalize beyond the training dataset.

To further characterize the model’s performance, a classification report was generated to compute precision, recall, and F1-score for each class. The results showed that all twelve object classes achieved precision and recall values ranging from approximately 93\% to 94\%, confirming balanced performance across different categories. No single class dominated the results, and no substantial drop-off in performance was observed for any specific object category.

Importantly, these results represent a significant improvement over the baseline performance. As calculated during evaluation methodology, simply guessing the most common class (water bottle) would yield a baseline accuracy of roughly 8.7\%. Achieving over 94\% accuracy with the CNN demonstrates the model's ability to learn complex visual distinctions between everyday objects.

\subsection{Analysis of Learning Trends and Model Behavior}

The convergence patterns observed during training and validation suggest that the model benefited substantially from the data augmentation process and the selected architectural choices. The minimal gap between training and validation metrics across epochs indicates that overfitting was well-controlled, and the network was capable of learning generalizable patterns rather than memorizing the training data.

While the model achieved strong results, small fluctuations in validation accuracy during the later epochs suggest that the model may have begun to overfit slightly toward the end of training. This is a common occurrence in deep learning and highlights the importance of techniques such as early stopping, additional regularization, and more extensive hyperparameter tuning in future experiments.

Additionally, given the relatively simple architecture used in this project compared to state-of-the-art image classifiers, the high level of achieved performance suggests that the classification task was solvable without the need for extremely deep or complex networks.

\subsection{Opportunities for Further Improvement}

Several opportunities exist for improving the model beyond the current results. Implementing a more aggressive regularization strategy—such as increasing the dropout rate, introducing L2 weight decay penalties, or applying early stopping based on validation loss—could further mitigate potential overfitting.

Additionally, experimenting with deeper architectures, such as adding more convolutional layers or increasing filter size, might improve the model's ability to extract fine-grained features from the images. Exploring different activation functions, such as Leaky ReLU or SELU, could also be beneficial.

Finally, transfer learning using pre-trained CNN models like VGG16, ResNet50, or MobileNet could provide a significant performance boost, especially when applied to relatively small custom datasets like the one used in this project. Leveraging these pre-trained networks would allow the model to benefit from feature representations learned from massive external datasets, potentially improving accuracy and training efficiency. 


%% TODO: write about what you conclude. This is not meant to be a summary section but more of a takeaways/future work section.
%% Must contain:
%% - Any concrete takeaways or conclusions from your experiments/results/project
%%
%% You can also discuss limitations here if you want.
%%
\section{Conclusions}

% TODO:
In this project, a convolutional neural network (CNN) was developed, trained, and evaluated for the task of image classification. Using a relatively simple architecture with two convolutional layers, max pooling, and dense layers, the model was able to achieve a final training accuracy of approximately 98\% and a validation accuracy of approximately 95\%. When evaluated on a separate test set, the model achieved an accuracy of approximately 94\%.

The precision, recall, and F1-score values for each class were consistently around 93--94\%, demonstrating that the model maintained balanced and accurate predictions across all categories. The use of data augmentation played a significant role in improving the model's ability to generalize to unseen examples.

Although the model performed well, there is potential for future improvements through techniques such as dropout regularization, deeper architectures, hyperparameter optimization, or the application of transfer learning with pre-trained networks. Nevertheless, the results demonstrate that even relatively simple CNNs, when properly trained, can achieve high levels of performance on complex image classification tasks. 

%%%%

\bibliography{refs}
\bibliographystyle{plain}


\end{document} % end tag of the document
