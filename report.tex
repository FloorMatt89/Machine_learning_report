\documentclass[10pt]{article}

\usepackage[left=0.8in,right=0.8in,top=0.15in,bottom=0.8in]{geometry}
\usepackage{xcolor}
\usepackage{hyperref}

\hypersetup{colorlinks=true,linkcolor=blue,urlcolor=blue}
\urlstyle{rm}
\usepackage{url}


\title{CAI 4104: Machine Learning Engineering\\
	\large Project Report:  {\textcolor{purple}{Image Classification Using a CNN}}} %% TODO: replace with the title of your project
	
	
	
%% TODO: your name and email go here (all members of the group)
%% Comment out as needed and designate a point of contact
%% Add / remove names as necessary
\author{
        Sebastian Villamizar \\
        villamizar.s@ufl.edu\\
        \and
        Jason Chen \\
        jchen8@ufl.edu\\
        \and
        Matthew Segura \\
        matthewsegura@ufl.edu\\
        \and
        Adrian Lehnhaeuser \\
        alehnhaeuser@ufl.edu\\
        \and
        Logan Dukes\\
        email5@ufl.edu\\
}


% set the date to today
\date{\today}


\begin{document} % start document tag

\maketitle



%%% Remember: writing counts! (try to be clear and concise.)
%%% Make sure to cite your sources and references (use refs.bib and \cite{} or \footnote{} for URLs).
%%%



%% TODO: write an introduction to make the report self-contained
%% Must address:
%% - What is the project about and what is notable about the approach and results?
%%
\section{Introduction}
The final project for CAI4104 is about implementing a complete ML pipeline for object classification from images of everyday objects. We classified 12 objects: pen, paper, book, clock, phone, laptop, chair, desk, water bottle, keychain, backpack, calculator. To accomplish this, we used a Convolutional Neural Network because it is well-suited to computer tasks on image data. CNNs have high memory usage during training so we leveraged computing power from HiPerGator and Google Colab. TODO: ADD SIMPLE RESULTS
% TODO:
%Write here. You can cite scholarly work like this~\cite{murphy2022probabilistic}. You can also include URLs in a footnote %like this.\footnote{UCI repository: \url{https://archive.ics.uci.edu/}.}




%% TODO: write about your approach / ML pipeline
%% Must contain:
%% - How you are trying to solve this problem
%% - How did you process the data?
%% - What is the task and approach (ML techniques)?
%%
\section{Approach}
To implement our pipeline we imported several python libraries: numpy, pandas, tensorflow, sklearn, and pytorch. These libraries helped us process the data and train the model. First, we applied label encoding to convert the object classes into integer class labels ranging from 0 to 11. These integer labels are compatible with the Sparse Categorical Cross Entropy loss function used in training our CNN model. To load and preprocess the image data we iterated over each image file and added them to a numpy array X, our feature matrix, and the labels to Y. Since the images have RGB values that range from 0 to 255, we normalized the data by dividing each pixel value by 255 to get them in the range [0, 1]. This is important because it accelerates convergence and improves numerical stability by stabilizing the gradients calculated by the optimizer, Adam. Then we used a standard seed, 42, and an 80, 10, 10 percent training, test, and validation split. We tested this data on a simple CNN architecture using tensorflow. 

The architecture consists of 3 convolutional layers with filter size 3x3, increasing channel depth (32, 64, 128), each with ReLU activation and max-pooling layers. This is followed by a flattening layer, a dense layer with 256 units, a drouput layer with a 0.5 rate, and finally a dense output layer with 12 units and softmax activation. After training this model over 30 epochs using the adam optimizer and SparseCategoricalCrossentropy loss function, we achieved a training accuracy of ~97\% and a loss of 0.0755. But when we evaluaed it on the unseen test data, the accuracy dropped significantly to 56\%. Therefore, our model was overfitting due to a lack of data augmentation. To augment the data we used a YouTube video by codebasics to guide us\footnote{Data augmentation to address overfitting: \url{https://www.youtube.com/watch?v=mTVf7BN7S8w&list=WL&index=2&t=1601s}.} For every input we generated multiple new samples using edge enhancement, smoothing, grayscale, gaussian blur, oversaturation, and color inversion. This increased amount of images from 4757 to 57084. 






%% TODO: write about your evaluation methodology
%% Must contain:
%% - What are the metrics?
%% - What are the baselines?
%% - How did you split the data?
%%
\section{Evaluation Methodology}

% TODO:
Write here. 




%% TODO: write about your results/findings
%% Must contain:
%% - results comparing your approach to baseline according to metrics
%%
%% You can present this information in whatever way you want but consider tables and (or) figures.
%%
\section{Results}

% TODO:
Write here. 





%% TODO: write about what you conclude. This is not meant to be a summary section but more of a takeaways/future work section.
%% Must contain:
%% - Any concrete takeaways or conclusions from your experiments/results/project
%%
%% You can also discuss limitations here if you want.
%%
\section{Conclusions}

% TODO:
Write here. 

%%%%

\bibliography{refs}
\bibliographystyle{plain}


\end{document} % end tag of the document
